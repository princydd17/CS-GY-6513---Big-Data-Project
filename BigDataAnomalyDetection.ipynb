{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set Environment accordingly\n",
    "os.environ['JAVA_HOME'] = \"/Library/Java/JavaVirtualMachines/adoptopenjdk-8.jdk/Contents/Home/\"\n",
    "os.environ['SPARK_HOME'] = \"/Users/simran/Downloads/spark-3.5.5-bin-hadoop3/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = \"./full_history\"\n",
    "file_pattern = \"*.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BigDataProject1\") \\\n",
    "    .config(\"spark.driver.bindAddress\", \"127.0.0.1\") \\\n",
    "    .config(\"spark.executor.memory\", \"12g\") \\\n",
    "    .config(\"spark.driver.memory\", \"12g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"4g\") \\\n",
    "    .config(\"spark.memory.offHeap.enabled\", True) \\\n",
    "    .config(\"spark.memory.offHeap.size\", \"20g\") \\\n",
    "    .config(\"spark.default.parallelism\", \"100\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .config(\"spark.executor.extraJavaOptions\", \"-XX:+UseG1GC\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.JavaSerializer\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, DateType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"date\", DateType(), True),\n",
    "    StructField(\"volume\", DoubleType(), True),  \n",
    "    StructField(\"open\", DoubleType(), True),\n",
    "    StructField(\"high\", DoubleType(), True),\n",
    "    StructField(\"low\", DoubleType(), True),\n",
    "    StructField(\"close\", DoubleType(), True),\n",
    "    StructField(\"adj close\", DoubleType(), True),\n",
    "    StructField(\"StockName\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets as DataFrames\n",
    "history_df = spark.read.format(\"csv\") \\\n",
    "     .option(\"header\", \"true\") \\\n",
    "     .option(\"treatEmptyValuesAsNulls\", \"true\") \\\n",
    "     .option(\"schema\",schema) \\\n",
    "     .load(f\"{directory_path}/{file_pattern}\")\n",
    "\n",
    "\n",
    "history_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "history_df = history_df.dropDuplicates()\n",
    "\n",
    "# Handle missing values for both open and close simultaneously\n",
    "history_df = history_df.na.fill({\n",
    "    \"open\": np.nan,\n",
    "    \"close\": np.nan\n",
    "})\n",
    "history_df = history_df.replace(float('nan'), np.nan)\n",
    "\n",
    "history_df.cache()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
